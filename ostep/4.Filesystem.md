操作系统基础 - 文件系统

# 前言

本文以一个非常简单的文件系统vsfs(Very Simple File System)为例，介绍文件系统实现需要注意的要素。我们可以从两个角度来看待文件系统：
1. 文件系统的数据结构是怎么组织的，文件数据和元数据在硬盘上是怎么存放的？
2. 访问文件系统的方法，当我们打开、读取或写入一个文件时，需要读写哪些数据结构？

# 数据结构

## 物理块和逻辑块

我们可以把一个硬盘看做是一个大的数组，每个数组成员的大小通常是512字节，这是磁盘控制器所能够读写的最小单元，称之为物理块。换句话说，假设计算机需要修改某个块中1个字节，磁盘控制器必须把整块512字节的内容读取出来，修改这1字节，再把512完整地写回磁盘中。而文件系统所能读写的最小块，称之为逻辑块，它的大小一般在512字节到16KiB之间，因此一个逻辑块会对应一个或多个连续的物理块，典型的逻辑块大小通常是是4KiB。假设这里有一个很小的只有256KiB的硬盘，我们可以把它划分成64个4KiB的逻辑块，编号0-63：

![](./img/blocks.png)

> 为什么逻辑块跟物理块的大小不同？为什么典型的逻辑块大小是4KiB？我没找到明确的结论，这可能跟page cache的大小有关，一个4KiB逻辑块的内容正好映射到4KiB的page cache中，从而简化了设计。实际上，现在越来越多硬盘也开始采用4KiB的物理块大小了。另一方面，4KiB是个常见的magic number，如果不知道多大合适，通常4KiB是个不坏的选择。

## 磁盘空间划分

不难想象，硬盘的大部分都应该做作为数据区（Data Region），用来保存文件的实际内容；同还需要一部分空间作为元数据来索引这些文件，最简单的做法就是按照比例来划分。

元数据通常会放在硬盘的起始部分，需要实现以下几点
- 用来索引数据块的inode，每个inode代表了一个文件或者目录。
- 记录数据块和inode是否已经分配的数据结构，这里我们采用了两个bitmap：
  - inode map（图中的i）记录inode的分配情况
  - data map（图中的d）记录数据库分配情况
- 最后是超级块，里面保存了文件系统的类型，inode和数据块的个数等信息。


![](./img/fsds.png)


## inode

每一个文件，比如/foo/bar，/abc在文件系统上都有inode描述它的元信息，inode是索引节点（index node）的简称，因为最早是以数组的形式来保存inode的，如下图所示。假如每个inode的大小是256，这5个4KiB的逻辑块一共可以保存80个inode。

![](./img/inode.png)

> 常见的文件系统会根据磁盘的大小，通过固定的比例来分配inode的数量，这样的好处是设计和实现和简单。我们有时候会遇到文件系统报磁盘满了的错误，而通过df命令却看到磁盘空间还很空闲的情况，这是因为inode用完了。像是ReiserFS之类的文件系统是动态分配inode的，它们不会出现这种情况。

inode中包含了文件所有除了内容以外的所有相关信息，以一个ext2的inode为例，它包含了以下字段：

| 大小 | 字段名      | 用途                           |
| ---- | ----------- | ------------------------------ |
| 2    | mode        | 这个文件是否可读/可写/可执行？ |
| 2    | uid         | 谁拥有这个文件？               |
| 4    | size        | 这个文件包含多少字节？         |
| 4    | time        | 文件的最后访问时间             |
| 4    | ctime       | 文件的创建时间                 |
| 4    | mtime       | 文件的最后修改时间             |
| 4    | dtime       | 这个inode是什么时候删除的      |
| 2    | gid         | 这个文件属于哪个用户组         |
| 2    | links_count | 这个文件一共有几个硬链接       |
| 4    | blocks      | 这个文件分配了多少个逻辑块     |
| 4    | flags       | ext2该怎么使用这个inode        |
| 4    | osd1        | 操作系统相关的字段             |
| 60   | block       | 一组指向硬盘的指针(一共15个)   |
| 4    | generation  | 文件版本（NFS使用）            |
| 4    | file_acl    | acl 是一种新的权限控制模型     |
| 5    | dir_acl     |

### 多级索引

在inode的设计中最重要的决策就是如何保存数据节点的磁盘地址，通用的做法是多级索引(Multi Level Index)，以ext2为例，它的15个block指针中，前12个叫做direct pointer，每一个都直接指向数据区中的一个4KiB的数据块，里面保存的是用户的文件数据。通过direct pointer一个indoe一共只能保存48KiB的数据，如果文件大小超过这个范围，ext2会分配一个indirect pointer（即第13个block指针），它指向一个特殊的数据块，里面保存的是一组指向用户文件数据的direct pointe。假设磁盘地址是4字节，一个indirect pointer可以包含1024个direct pointer，一共可以保存4096KiB的数据。

如果文件大小继续增长的话，ext2还会分配double indrect pointer（即第14个block指针） 和 triple indrect pointer（即第15个block指针）,如下图所示：

![](./img/800px-Ext2-inode.png)

为什么要用这种多级索引的设计？根本原因是大部分文件都是很小的，Agrawal等人在2007年做过一个调查发现：
|                            |                                                           |
| -------------------------- | --------------------------------------------------------- |
| 大部分文件都是很小         | 最常见的大小在2KiB左右                                    |
| 文件的平均大小正在增长     | 平均文件大小约200KiB                                      |
| 大部分的数据都存在大文件中 | 几个大文件占用了多数的空间                                |
| 文件系统包含很多文件       | 平均约有10万个                                            |
| 文件系统大约只使用了一半   | 虽然磁盘的大小在增长，但是文件系统保持了约50%的空间使用率 |
| 目录通常很小               | 很多目录只有几个文件，大部分目录的文件数在20个以下        |

> 作为对比，ext4采用了extents的方案，一个extent就是一个磁盘指针加上一个（逻辑块的）长度，一个文件通常会包含多个extent。读者朋友不妨思考下这两种方式在不同场景下的优劣。

## 目录

文件系统中的目录也对应一个inode，它指向的数据块中包含了文件的索引，在vsfs中它是一个简单的列表。表中的每一项都包含以下内容：
- inum: 文件的inode number
- reclen：该记录的长度
- strlen：文件名的长度
- name：以及实际的文件名

假设目录`dir`对应的inode number为5，并且它包含3个文件（foo，bar和foobar_is_a_pretty_longname)，它对应的记录项如下：

| inum | reclen | strlen | name                        |
| ---- | ------ | ------ | --------------------------- |
| 5    | 12     | 2      | .                           |
| 2    | 12     | 3      | ..                          |
| 12   | 12     | 4      | foo                         |
| 13   | 12     | 4      | bar                         |
| 24   | 36     | 28     | foobar_is_a_pretty_longname |

其中开头的一项名字为`.`，其inum也指向5，表示这个目录本身。第二项`..`指向其父目录。

# 访问路径

介绍完vsfs的数据结构后，我们来看看访问一个文件需要经过什么路径，下面以读取和写入一个文件举例：

## 读取文件

假设我们要打开一个叫/foo/bar的文件，然后发起3次读请求，为了简单起见，假设每次读取的大小都是4KiB（即逻辑块大小），其时间线如下：

![](./img/read.png)

首先读取根目录的inode（通常是一个固定的位置，也可能包含在超级块的信息中），根据inode的block指针从中读取根目录的数据块，从这个数据块中找到foo目录对应inode number。接下来读取foo目录的inode，然后读取其的数据块，从中找到bar文件的inode number。最后读取bar文件的inode，完成了打开文件(open)的过程。

当发起read()系统调用读取bar的内容时，首先从inode中找到它第一个block指针，把对应的内容读入内存，然后文件系统会更新inode中的最后访问时间（atime），因此触发了一次write操作。如此类推，最终完成3个逻辑块的读取。

## 写入文件

类似的，假如我们要在/foo目录下面创建bar文件，再发起3次大小为4KiB的write请求，其时间线如下：

![](./img/write.png)

首先读取根目录的inode（通常是一个固定的位置，也可能包含在超级块的信息中），根据inode的block指针从中读取根目录的数据块，从这个数据块中找到foo目录对应inode number，接下来读取foo目录的inode，然后读取其的数据块。到此为止，这里的过程跟读取文件时一致的。

接下来需要给bar文件分配一个inode，因此我们先读取inode bitmap，找到一个空闲inode number，把这个位置标记为已分配，然后写回bitmap。有了inode number之后，文件系统需要再/foo的记录表中添加foo文件的项，因此触发了一个write操作。最后，需要把foo的inode信息写入inode table中，由于inode的大小（通常为128或256字节)小于物理块大小（通常为512字节），因此还需要把inode所在得物理块内容读取进来写入foo文件的inode再写回磁盘中。至此完成了文件创建的过程。

当发起write()系统调用往bar文件写入内容时，首先要从inode的block指针中找到最后一个块的位置（对于刚创建的文件就是第一个指针）。接下来文件系统需要为bar文件分配一个数据块，因此读入data bitmap，找到一个空闲的逻辑块，标记为使用后写回。有了这个逻辑块位置之后，首先往这个位置写入数据内容， 最后更新indoe的block指针，这样inode就跟数据块关联起来了。

# Fast File System

最早的unix文件系统跟vsfs很像，它有一个很严重的问题，随着时间的推移，文件系统最后只能利用硬盘2%的带宽。为什么会这样？我们知道机械硬盘读写一个块的时候，需要一个寻道和旋转的时间才能最终发起请求。在vsfs中，inod位于磁盘的最外圈，数据块在内圈，写入数据的时候要多次寻道，因此无法发挥磁盘的性能。

Fast File System把磁盘分为多个块组（block group），每个组都包含有inode等元数据和数据块，当为inode分配数据块的时候尽可能把分在同一个块组或者临近的块组，通过空间局部性原理解决了这个问题。Linux的EXT系列文件系统也采用这种设计。

![](./img/bgs.png)
*块组*

![](./img/bg.png)
*一个块组*

# 数据一致性

从前面的描述中我们知道，在文件系统新写入一个数据块需要发起多次IO，如果在这个过程中因为断电，内核bug等各种原因导致这些数据没有完整的写入，会导致文件系统的数据不一致。来看一个例子，这里有一个很小的块组，一共8个inode和8个数据块，其中inode number 3 代表了一个文件，文件的内容指向数据块4，大小为4KiB。

![](./img/crash1.png)


假设我们要往文件中追加4KiB的内容，文件系统需要更新3个位置的数据：inode bitmap，inode，以及新分配的数据块，正常情况下所有数据写进去之后，新的块组分配情况如下：

![](./img/crash2.png)

然而由于文件系统设计，或是写缓存的原因，磁盘可能会以不同的顺序写入这些位置，如果在这个过程中断电，会导致不同的结果。

| dmap | inode | 数据块 | 结果                                                                                        |
| ---- | ----- | ------ | ------------------------------------------------------------------------------------------- |
| ok   | ok    | ok     | 这是正常的情况，文件系统数据一致                                                            |
| ok   | ok    | fail   | 数据未写入，读取的时候会读取到未初始化的脏数据                                              |
| ok   | fail  | ok     | dmap和数据块以分配却未被inode引用，造成磁盘空间泄漏                                         |
| ok   | fail  | fail   | dmap引用的数据块分配了却未被inode引用，造成磁盘空间泄漏                                     |
| fail | ok    | ok     | inode引用了数据块，其内容也是正确的，但是dmap认为这个数据块未使用，存在被其它数据覆盖的风险 |
| fail | ok    | fail   | inode引用了数据块，但内容是脏数据，同时dmap认为这个数据块未使用，存在被其它数据覆盖的风险   |
| fail | fail  | ok     | 仅写入数据块，但是未被dmap和inode引用，相当于这个写入丢失了，但但是文件系统数据一致         |
| fail | fail  | fail   | 没有任何数据写入，文件系统数据一致                                                          |

在最后两个场景中，文件系统数据是一致。然而从应用的角度来说，它未必是如此，比如应用发起写请求自之后回复客户端写入成功。

## Fsck

## Journal
